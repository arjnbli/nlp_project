{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train Pytorch",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN7nZIRfP1DoQ//wSjeLTJd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taoyilee/nlp_project/blob/colab/colab/Train_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVlatDCmJIK4",
        "colab_type": "text"
      },
      "source": [
        "Setup dataset from Big Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrwM98HiJNsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyC8mRHiJb7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MomrKSAKF5na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SlJL6nMLnAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2DoubleHeadsModel, GPT2Tokenizer, PreTrainedTokenizer\n",
        "model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'sep_token': '[SEP]','cls_token': '[CLS]'})\n",
        "model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpA1TgJzcSdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F74-WJsvPH9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N96RxMPZcQYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Vdk4_qI_NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BigQueryDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, project_name=\"focus-empire-270208\", table_name=\"asnq.train\", block_size=512):        \n",
        "        print(f\"Creating features from table {project_name}.{table_name}\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.project_name = project_name\n",
        "        self.table_name = table_name\n",
        "        self.client = bigquery.Client(project = self.project_name)\n",
        "        self.block_size=block_size\n",
        "\n",
        "    def __len__(self):        \n",
        "        QUERY = ('SELECT '\n",
        "          'COUNT(*) as total_rows '\n",
        "          f'FROM `{self.table_name}`')\n",
        "        query_job = self.client.query(QUERY)  \n",
        "        rows = query_job.result().to_dataframe()                \n",
        "        return rows.loc[0, \"total_rows\"]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        QUERY = ('SELECT * '          \n",
        "            f'FROM `{self.table_name}` '            \n",
        "            f'LIMIT 1 OFFSET {i}')        \n",
        "        query_job = self.client.query(QUERY)          \n",
        "\n",
        "        rows = query_job.result().to_dataframe().loc[0]\n",
        "        x = f\"{rows['question']} [SEP] {rows['context']} [CLS]\"\n",
        "                  \n",
        "        return torch.LongTensor(self.tokenizer.encode(x, max_length=self.block_size), dtype=torch.long), rows['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99W-_44UG-Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80V8yptHk6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount(\"/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j07FxqpsIJQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/gdrive/My Drive/UCI/06_Winter_2020/cs272_nlp/output/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9YYHBkWSv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " max_steps = 0\n",
        " gradient_accumulation_steps = 1\n",
        " num_train_epochs=1\n",
        " weight_decay=0\n",
        " learning_rate=5e-5\n",
        " adam_epsilon=1e-8\n",
        " warmup_steps=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTs_W50VWZLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMod_3NIWsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        num_train_epochs = args.max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
        "else:\n",
        "        t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "    },\n",
        "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqsHnYtvXc9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWzX-rgbXoRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)    \n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_su0XlibWcxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs_trained = 0\n",
        "model.zero_grad()\n",
        "train_iterator = trange(epochs_trained, int(num_train_epochs), desc=\"Epoch\")\n",
        "set_seed(0)  # Added here for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-fLHyEqZF3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenizer.bos_token)\n",
        "print(tokenizer.eos_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibSlsT93TzEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 4\n",
        "def collate(examples):   \n",
        "    return pad_sequence([e[0] for e in examples], batch_first=True), torch.LongTensor([e[1] for e in examples] )\n",
        "\n",
        "train_dataset = BigQueryDataset(tokenizer,table_name=\"asnq.train\")\n",
        "dev_dataset = BigQueryDataset(tokenizer,table_name=\"asnq.dev\")\n",
        "train_sampler = RandomSampler(train_dataset) \n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, collate_fn=collate )\n",
        "for i in train_dataloader:\n",
        "  print(i[0].shape, i[1].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTKS_hCuXfM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in train_iterator:\n",
        "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "    for step, (batch, batch_mc) in enumerate(epoch_iterator):\n",
        "        inputs, lm_labels, mc_labels = (batch, batch, batch_mc)\n",
        "        print(inputs.shape)\n",
        "        print(lm_labels.shape)\n",
        "        print(mc_labels.shape)\n",
        "        inputs = inputs.to(device)        \n",
        "        lm_labels = lm_labels.to(device)\n",
        "        mc_labels = mc_labels.to(device)\n",
        "        model.train()\n",
        "        outputs =  model(inputs, lm_labels=lm_labels, mc_labels=mc_labels)\n",
        "        loss = outputs[0] + outputs[1] # model outputs are always tuple in transformers (see doc)\n",
        "    \n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:    \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "    #         if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "    #             # Log metrics\n",
        "    #             if (\n",
        "    #                 args.local_rank == -1 and args.evaluate_during_training\n",
        "    #             ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "    #                 results = evaluate(args, model, tokenizer)\n",
        "    #                 for key, value in results.items():\n",
        "    #                     tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "    #             tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "    #             tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "    #             logging_loss = tr_loss\n",
        "\n",
        "    #         if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "    #             checkpoint_prefix = \"checkpoint\"\n",
        "    #             # Save model checkpoint\n",
        "    #             output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "    #             os.makedirs(output_dir, exist_ok=True)\n",
        "    #             model_to_save = (\n",
        "    #                 model.module if hasattr(model, \"module\") else model\n",
        "    #             )  # Take care of distributed/parallel training\n",
        "    #             model_to_save.save_pretrained(output_dir)\n",
        "    #             tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    #             torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "    #             logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "    #             _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "    #             torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "    #             torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "    #             logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "    #     if args.max_steps > 0 and global_step > args.max_steps:\n",
        "    #         epoch_iterator.close()\n",
        "    #         break\n",
        "    # if args.max_steps > 0 and global_step > args.max_steps:\n",
        "    #     train_iterator.close()\n",
        "    #     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOatissJnF_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}